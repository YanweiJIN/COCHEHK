{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "import scipy.io\n",
    "data = scipy.io.loadmat('/Users/jinyanwei/Desktop/BP_Model/Data/Cuffless_BP_Estimation/part_1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs =125 # Sample rate in Hz\n",
    "\n",
    "import scipy.signal as signal\n",
    "def chebyshev_filter(signal):\n",
    "    # Define the filter order and cutoff frequency\n",
    "    order = 4\n",
    "    cutoff_freq = 20  # Cutoff frequency in Hz\n",
    "    # Create the Chebyshev low-pass filter\n",
    "    b, a = signal.cheby1(order, 0.5, cutoff_freq / (fs / 2), 'low', analog=False)\n",
    "    return signal.lfilter(b, a, signal)\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "def butter_filter(signal):\n",
    "    fs = 125  # Sample rate\n",
    "    cutoff = 5  # Cutoff frequency in Hz\n",
    "    # Design the Butterworth low-pass filter\n",
    "    nyquist = 0.5 * fs\n",
    "    cutoff_norm = cutoff / nyquist\n",
    "    b, a = butter(4, cutoff_norm, btype='low')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "import numpy as np\n",
    "def straighten_ecg(ecg_signal):\n",
    "    detrended_ecg = np.subtract(ecg_signal, np.mean(ecg_signal))\n",
    "    return detrended_ecg\n",
    "    \n",
    "import numpy as np\n",
    "def normalize_sinal(ppg):\n",
    "# Assuming ppg_signal and ecg_signal are your original PPG and ECG signals\n",
    "    ppg_min = np.min(ppg)\n",
    "    ppg_max = np.max(ppg)\n",
    "    normalized_ppg = (ppg - ppg_min) / (ppg_max - ppg_min)\n",
    "    return normalized_ppg\n",
    "def standard_signal(bp): \n",
    "    return (bp - np.mean(bp)) / np.std(bp)\n",
    "def inverse_standard_signal(bp_ori, bp_est):\n",
    "    mean = np.mean(bp_ori)\n",
    "    std = np.std(bp_ori)\n",
    "    return (bp_est * std) + mean\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "def align_ppgbp_segment(ppg_signal, bp_signal1, bp_signal2, ecg_signal, show=0): ## ppg_signal = ppg_normalized, bp_signal = bp_normalized, ecg_signal = ecg_normalized; get ppg_aligned, bp_aligned\n",
    "    ppg_peaks, _ = find_peaks(ppg_signal, height=0.5)  # Adjust the height threshold as needed\n",
    "    bp_peaks, _ = find_peaks(bp_signal1, height=0.4)\n",
    "    ecg_peaks, _ = find_peaks(ecg_signal, height=0.65)\n",
    "    #print(f'ppg peaks: {len(ppg_peaks)} {ppg_peaks}')\n",
    "    #print(f'ecg peaks: {len(ecg_peaks)} {ecg_peaks}')\n",
    "\n",
    "    first_ecg_peak = ecg_peaks[0]\n",
    "    #print(f'first ecg peak: {first_ecg_peak}')\n",
    "    indices_ppg = np.argwhere(ppg_peaks[:10] > first_ecg_peak)\n",
    "    first_ppg_peak = ppg_peaks[int(indices_ppg[0])]\n",
    "    #print(f'first ppg peak: {first_ppg_peak}')\n",
    "    indices_bp = np.argwhere(bp_peaks[:10] > first_ecg_peak)\n",
    "    first_bp_peak = bp_peaks[int(indices_bp[0])]\n",
    "    #print(f'first bp peak: {first_bp_peak}')\n",
    "    ppg_bp_peaks_subtraction = abs(bp_peaks[int(indices_bp[0]):int(indices_bp[0])+20] - ppg_peaks[int(indices_ppg[0]):int(indices_ppg[0])+20])\n",
    "    #print(ppg_bp_peaks_subtraction)\n",
    "    distance_ppgbp = np.bincount(ppg_bp_peaks_subtraction).argmax()\n",
    "    #print(move_distance)\n",
    "    #print(bp_peaks[int(indices_bp[0]):int(indices_bp[0])+20] - ppg_peaks[int(indices_ppg[0]):int(indices_ppg[0])+20])\n",
    "    #print(f'distance:{distance_ppgbp}')\n",
    "    if first_bp_peak > first_ppg_peak:\n",
    "        bp_aligned = bp_signal1[distance_ppgbp:]\n",
    "        bp_ori_aligned = bp_signal2[distance_ppgbp:]\n",
    "        ppg_aligned = ppg_signal\n",
    "    elif first_bp_peak < first_ppg_peak:\n",
    "        bp_aligned = bp_signal1\n",
    "        bp_ori_aligned = bp_signal2\n",
    "        ppg_aligned = ppg_signal[distance_ppgbp:]\n",
    "    else:\n",
    "        bp_aligned = bp_signal1\n",
    "        bp_ori_aligned = bp_signal2\n",
    "        ppg_aligned = ppg_signal\n",
    "    #print(f'ppg len: {len(ppg_aligned)}')\n",
    "    #print(f'bp len: {len(bp_aligned)}')\n",
    "    min_len = min(len(bp_aligned), len(ppg_aligned))\n",
    "    ppg_aligned = ppg_aligned[:min_len]\n",
    "    bp_aligned = bp_aligned[:min_len]\n",
    "    bp_ori_aligned = bp_ori_aligned[:min_len]\n",
    "    ecg_aligned = ecg_signal[:min_len]\n",
    "    #print(ecg_aligned)\n",
    "    ppg_segmented = ppg_aligned[:first_ecg_peak-5]\n",
    "    bp_segmented = bp_aligned[:first_ecg_peak-5]\n",
    "    bp_ori_segmented = bp_ori_aligned[:first_ecg_peak-5]\n",
    "    ecg_segmented = ecg_aligned[:first_ecg_peak-5]\n",
    "\n",
    "    for ecgi in range(len(ecg_peaks)-1):\n",
    "        one_ppg_peak, _ = find_peaks(ppg_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5], height=0.5)\n",
    "        #print(ecg_peaks[ecgi], one_ppg_peak)\n",
    "        if len(one_ppg_peak) == 1:\n",
    "            ppg_segmented = np.concatenate((ppg_segmented, ppg_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5]))\n",
    "            bp_segmented = np.concatenate((bp_segmented, bp_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5]))\n",
    "            bp_ori_segmented = np.concatenate((bp_ori_segmented, bp_ori_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5]))\n",
    "            ecg_segmented = np.concatenate((ecg_segmented, ecg_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5]))\n",
    "\n",
    "    if show == 1:\n",
    "        plt.figure(figsize=(30, 6))\n",
    "        plt.plot(ppg_signal, label='PPG')\n",
    "        #plt.plot(bp_signal1, label='BP')\n",
    "        #plt.plot(ecg_signal, label='ECG')\n",
    "        plt.scatter(ppg_peaks, ppg_signal[ppg_peaks], color='c', marker='o', label='Aligned PPG Peaks')\n",
    "        #plt.scatter(bp_peaks, bp_signal1[bp_peaks], color='orange', marker='o', label='Aligned BP Peaks')\n",
    "        #plt.scatter(ecg_peaks, ecg_signal[ecg_peaks], color='green', marker='o', label='Aligned BP Peaks')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(30, 6))\n",
    "        plt.plot(ppg_segmented, label='PPG')\n",
    "        #plt.plot(bp_segmented, label='BP')\n",
    "        #plt.plot(ecg_segmented, label='ECG')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return ppg_segmented, bp_segmented, bp_ori_segmented, ecg_segmented\n",
    "\n",
    "import numpy as np\n",
    "def get_feautres(ppg_signal, bp_signal1, bp_signal2, ecg_signal):\n",
    "    ori_ecg_peaks, _ = find_peaks(ecg_signal)\n",
    "    r_peaks = np.array([ecg_peak for ecg_peak in ori_ecg_peaks if ecg_signal[ecg_peak] > 0.65])\n",
    "    #print(f'r_peaks: {len(r_peaks)} {r_peaks}')\n",
    "    r_peak_amplitudes = (ecg_signal[r_peaks]).tolist()\n",
    "    r_peak_amplitudes = r_peak_amplitudes[:-1]\n",
    "    #print(f'r_peak_amplitudes: {len(r_peak_amplitudes)} {r_peak_amplitudes}')\n",
    "    r_peak_intervals = (np.diff(r_peaks) / fs).tolist()\n",
    "    #print(f'r_peak_intervals: {len(r_peak_intervals)} {r_peak_intervals}')\n",
    "    # calculate low peak, s-peak\n",
    "    low_peak_amplitudes, r_to_low_peak_amplitudes, s_peak_amplitudes= [], [], []\n",
    "    low_peaks, s_peaks = [], []\n",
    "    for i in range(len(r_peaks) - 1):\n",
    "        r_peak = r_peaks[i]\n",
    "        next_r_peak = r_peaks[i + 1]\n",
    "        low_peak_amplitude = np.min(ecg_signal[r_peak:next_r_peak])\n",
    "        r_to_low_peak_amplitude = ecg_signal[r_peak]-low_peak_amplitude\n",
    "        low_peak_amplitudes.append(low_peak_amplitude)\n",
    "        r_to_low_peak_amplitudes.append(r_to_low_peak_amplitude)\n",
    "        low_peak = r_peak + np.argmin(ecg_signal[r_peak:next_r_peak])\n",
    "        low_peaks.append(low_peak)\n",
    "        s_peak_amplitude = np.min(ecg_signal[r_peak:low_peak])\n",
    "        s_peak_amplitudes.append(s_peak_amplitude)\n",
    "        s_peak = r_peak + np.argmin(ecg_signal[r_peak:low_peak])\n",
    "        s_peaks.append(s_peak)   \n",
    "    #print(f'low peaks: {len(low_peaks)} {low_peaks}')  \n",
    "    #print(f's peaks: {len(s_peaks)} {s_peaks}')  \n",
    "    # T-Wave Amplitude Calculation\n",
    "    r_peaks = np.insert(r_peaks, 0, 0) #the first t-peak is before the R-peak\n",
    "    t_wave_amplitudes, q_wave_amplitudes = [], []\n",
    "    t_peaks, q_peaks = [], []\n",
    "    for i in range(len(r_peaks) - 1):\n",
    "        r_peak = r_peaks[i]\n",
    "        next_r_peak = r_peaks[i + 1]\n",
    "        t_wave_amplitude = np.max(ecg_signal[r_peak:next_r_peak])\n",
    "        t_wave_amplitudes.append(t_wave_amplitude)\n",
    "        t_peak = r_peak + np.argmax(ecg_signal[r_peak:next_r_peak])\n",
    "        t_peaks.append(t_peak)\n",
    "        q_wave_amplitude = np.min(ecg_signal[t_peak:next_r_peak])\n",
    "        q_wave_amplitudes.append(q_wave_amplitude)\n",
    "        q_peak = r_peak + np.argmin(ecg_signal[t_peak:next_r_peak])\n",
    "        q_peaks.append(q_peak)\n",
    "\n",
    "    t_wave_amplitudes = t_wave_amplitudes[:-1]\n",
    "    q_wave_amplitudes = q_wave_amplitudes[:-1]\n",
    "    #print(f't peaks: {len(t_peaks)} {t_peaks}')  \n",
    "    #print(f'q peaks: {len(q_peaks)} {q_peaks}')  \n",
    "    \n",
    "    # QRS interval\n",
    "    r_peaks = np.array([ecg_peak for ecg_peak in ori_ecg_peaks if ecg_signal[ecg_peak] > 0.65])\n",
    "    qrs_intervals = []\n",
    "    for i in range(len(r_peaks)-1):\n",
    "        qrs_interval = (ecg_signal[s_peaks[i]] - ecg_signal[q_peaks[i]]) / fs\n",
    "        qrs_intervals.append(abs(qrs_interval))\n",
    "    #print(f'qrs_intervals: {len(qrs_intervals)} {qrs_intervals}')  \n",
    "\n",
    "    # get ppg features:\n",
    "    ppg_pulses, bp_pulses, bp_ori_pulses = [], [], []\n",
    "    for i in range(len(r_peaks)-1):\n",
    "        ppg_pulse = ppg_signal[r_peaks[i]:r_peaks[i+1]]\n",
    "        bp_pulse = bp_signal1[r_peaks[i]:r_peaks[i+1]]\n",
    "        bp_ori_pulse = bp_signal2[r_peaks[i]:r_peaks[i+1]]\n",
    "        ppg_pulses.append(ppg_pulse)\n",
    "        bp_pulses.append(bp_pulse)\n",
    "        bp_ori_pulses.append(bp_ori_pulse)\n",
    "    #print(f'bp pulses: {bp_pulses}')\n",
    "    ppg_pulse_amplitude = [np.max(pulse) - np.min(pulse) for pulse in ppg_pulses]\n",
    "    ppg_pulse_width = [pulse.shape[0]/fs for pulse in ppg_pulses]\n",
    "    ppg_high_to_low_interval = [np.argmin(pulse)/fs for pulse in ppg_pulses]\n",
    "    ppg_slope_change_std = [np.std(np.diff(pulse)) for pulse in ppg_pulses]\n",
    "\n",
    "    sbp = [np.max(pulse) for pulse in bp_pulses]\n",
    "    dbp = [np.min(pulse) for pulse in bp_pulses]\n",
    "    sbp_ori = [np.max(pulse) for pulse in bp_ori_pulses]\n",
    "    dbp_ori = [np.min(pulse) for pulse in bp_ori_pulses]\n",
    "    \n",
    "    return sbp, dbp, sbp_ori, dbp_ori, ppg_pulse_amplitude, ppg_pulse_width, ppg_high_to_low_interval, ppg_slope_change_std, t_wave_amplitudes, q_wave_amplitudes, r_peak_amplitudes, s_peak_amplitudes, low_peak_amplitudes, qrs_intervals, r_to_low_peak_amplitudes, r_peak_intervals\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pandas as pd\n",
    "def create_sequences(X, y, time_steps=4):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)])\n",
    "        ys.append(y.iloc[i+1])\n",
    "    return np.array(Xs), np.array(ys)  \n",
    "\n",
    "def bp_lstm_model(train_features, train_bp):\n",
    "    time_steps = 4\n",
    "    X_train, y_train = create_sequences(pd.DataFrame(train_features), pd.DataFrame(train_bp), time_steps)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=100, return_sequences=True, input_shape=(time_steps, 12)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=100))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=200, verbose=1)\n",
    "    return model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "def gru_model(X_train, y_train):\n",
    "    time_steps = 4\n",
    "    X_train, y_train = create_sequences(pd.DataFrame(X_train), pd.DataFrame(y_train), time_steps)\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=100, return_sequences=True, input_shape=(time_steps, 12)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=100, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=100))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=300, verbose=1)\n",
    "    return model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show_one(signal1):\n",
    "    fig = plt.figure(figsize=(30,6))\n",
    "    plt.plot(signal1)\n",
    "    return plt.show()\n",
    "def show_two(signal1, signal2):\n",
    "    fig = plt.figure(figsize=(30,6))\n",
    "    plt.plot(signal1, label='1')\n",
    "    plt.plot(signal2, label='2')\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "def show_three(signal1, signal2, signal3):\n",
    "    fig = plt.figure(figsize=(30,6))\n",
    "    plt.plot(signal1, label='1')\n",
    "    plt.plot(signal2, label='2')\n",
    "    plt.plot(signal3, label='3')\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all patient dataset\n",
    "patients = [0, 1, 2, 3, 4, 6, 14, 15, 16, 18, 24, 35, 36, 37, 39, 40, 41, 46, 64, 66, 67, 73, 79, 80, 81, 82, 86, 87, 88, 89, 93, 94, 95, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 115, 116, 117, 118, 120, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 141, 144, 149, 150, 151, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 179, 182, 184, 186, 188, 189, 192, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 272, 273, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 295, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 313, 314, 315, 316, 317, 318, 319, 320, 321, 323, 324, 327, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 377, 380, 391, 393, 394, 395, 397, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 413, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 429, 430, 431, 432, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 459, 460, 462, 463, 464, 465, 466, 470, 471, 472, 475, 480, 481, 482, 483, 484, 486, 487, 488, 490, 491, 492, 496, 497, 498, 499, 500, 502, 503, 504, 505, 507, 508, 509, 510, 512, 514, 515, 516, 520, 524, 527, 528, 529, 530, 531, 532, 533, 535, 537, 538, 539, 543, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 564, 565, 570, 576, 577, 578, 582, 590, 592, 593, 597, 599, 600, 602, 603, 604, 606, 607, 609, 612, 625, 626, 627, 628, 629, 630, 633, 634, 635, 639, 640, 641, 644, 646, 648, 653, 655, 656, 657, 658, 662, 670, 673, 674, 676, 677, 681, 682, 683, 685, 686, 687, 688, 689, 692, 693, 695, 696, 697, 698, 700, 703, 704, 705, 706, 707, 708, 709, 711, 712, 717, 719, 720, 721, 722, 723, 724, 725, 727, 729, 730, 731, 733, 734, 735, 736, 737, 741, 743, 744, 748, 749, 753, 754, 755, 756, 759, 774, 775, 776, 777, 780, 781, 783, 784, 785, 786, 787, 790, 791, 792, 794, 799, 800, 801, 802, 803, 805, 807, 808, 809, 810, 813, 820, 821, 822, 823, 828, 829, 830, 843, 845, 847, 868, 869, 922, 923, 947, 948, 949, 951, 952, 957, 959, 960, 962, 972, 975, 981, 989]\n",
    "sbp500 = []\n",
    "dbp500 = []\n",
    "sbp_ori500 = []\n",
    "dbp_ori500 = []\n",
    "features500 = []\n",
    "for patient in patients:\n",
    "    patient_data = data['p'][0,patient]\n",
    "    if len(patient_data[0]) >= 25000:\n",
    "        ppg_ori = patient_data[0]\n",
    "        bp_ori = patient_data[1]\n",
    "        ecg_ori = patient_data[2]\n",
    "        ecg_detrened = straighten_ecg(ecg_ori)\n",
    "        ppg_normalized = normalize_sinal(ppg_ori)\n",
    "        bp_standarded = bp_ori / 250\n",
    "        ecg_normalized = normalize_sinal(ecg_detrened)\n",
    "        ppg_segmented, bp_segmented, bp_ori_segmented, ecg_segmented = align_ppgbp_segment(ppg_signal = ppg_normalized, bp_signal1 = bp_standarded, bp_signal2 = bp_ori, ecg_signal = ecg_normalized, show=0)\n",
    "        bps_features = get_feautres(ppg_segmented, bp_segmented, bp_ori_segmented, ecg_segmented)\n",
    "        sbp = np.array(bps_features[0])\n",
    "        dbp = np.array(bps_features[1])\n",
    "        sbp_ori = np.array(bps_features[2])\n",
    "        dbp_ori = np.array(bps_features[3])\n",
    "        features = np.array(bps_features[4:])\n",
    "        sbp500.append(sbp)\n",
    "        dbp500.append(dbp)\n",
    "        sbp_ori500.append(sbp_ori)\n",
    "        dbp_ori500.append(dbp_ori)\n",
    "        features500.append(features)\n",
    "        print(patient)\n",
    "len(sbp_ori500)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train and test dataset\n",
    "train_sbp = sbp500[:400]\n",
    "train_dbp = dbp500[:400]\n",
    "train_sbp_ori = sbp_ori500[:400]\n",
    "train_dbp_ori = dbp_ori500[:400]\n",
    "train_features = features500[:400]\n",
    "test_sbp = []\n",
    "test_dbp = []\n",
    "test_sbp_ori = []\n",
    "test_dbp_ori = []\n",
    "test_features = []\n",
    "\n",
    "for i in range(len(sbp500)-400):\n",
    "    train_sbp.append(sbp500[i+400][:int(0.2*len(sbp500[i+400]))])\n",
    "    train_dbp.append(dbp500[i+400][:int(0.2*len(dbp500[i+400]))])\n",
    "    train_sbp_ori.append(sbp_ori500[i+400][:int(0.2*len(sbp_ori500[i+400]))])\n",
    "    train_dbp_ori.append(dbp_ori500[i+400][:int(0.2*len(dbp_ori500[i+400]))])\n",
    "    train_features.append(features500[i+400][:,:int(0.2*(features500[i+400].shape[1]))])\n",
    "    test_sbp.append(sbp500[i+400][int(0.2*len(sbp500[i+400])):])\n",
    "    test_dbp.append(dbp500[i+400][int(0.2*len(dbp500[i+400])):])\n",
    "    test_sbp_ori.append(sbp_ori500[i+400][int(0.2*len(sbp_ori500[i+400])):])\n",
    "    test_dbp_ori.append(dbp_ori500[i+400][int(0.2*len(dbp_ori500[i+400])):])\n",
    "    test_features.append(features500[i+400][:,int(0.2*(features500[i+400].shape[1])):])\n",
    "\n",
    "merged_train_sbp = np.concatenate(train_sbp, axis=0)\n",
    "merged_train_dbp = np.concatenate(train_dbp, axis=0)\n",
    "merged_train_sbp_ori = np.concatenate(train_sbp_ori, axis=0)\n",
    "merged_train_dbp_ori = np.concatenate(train_dbp_ori, axis=0)\n",
    "merged_train_features = np.concatenate(train_features, axis=1)\n",
    "merged_train_features = merged_train_features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_sbp = bp_lstm_model(merged_train_features, merged_train_sbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_dbp = bp_lstm_model(merged_train_features, merged_train_dbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model_sbp = gru_model(merged_train_features, merged_train_sbp)\n",
    "gru_model_dbp = gru_model(merged_train_features, merged_train_dbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eveluate the model\n",
    "mae_sbp_lstms = []\n",
    "rmse_sbp_lstms = []\n",
    "mae_sbp_grus = []\n",
    "rmse_sbp_grus = []\n",
    "mae_dbp_lstms = []\n",
    "rmse_dbp_lstms = []\n",
    "mae_dbp_grus = []\n",
    "rmse_dbp_grus = []\n",
    "for i in range(1):#len(test_sbp)\n",
    "    X_test, sbp_test = create_sequences(pd.DataFrame(test_features[i].T), pd.DataFrame(test_sbp[i]))\n",
    "    X_test, sbp_ori_test = create_sequences(pd.DataFrame(test_features[i].T), pd.DataFrame(test_sbp_ori[i]))\n",
    "    X_test, dbp_test = create_sequences(pd.DataFrame(test_features[i].T), pd.DataFrame(test_dbp[i]))\n",
    "    X_test, dbp_ori_test = create_sequences(pd.DataFrame(test_features[i].T), pd.DataFrame(test_dbp_ori[i]))\n",
    "    sbp_pred_lstm = lstm_model_sbp.predict(X_test)\n",
    "    sbp_ori_pred_lstm = sbp_pred_lstm * 250\n",
    "    mae_sbp_lstm = np.mean(np.abs(sbp_ori_test - sbp_ori_pred_lstm))\n",
    "    rmse_sbp_lstm = np.sqrt(np.mean((sbp_ori_test - sbp_ori_pred_lstm)**2))\n",
    "    dbp_pred_lstm = lstm_model_dbp.predict(X_test)\n",
    "    dbp_ori_pred_lstm = dbp_pred_lstm * 250\n",
    "    mae_dbp_lstm = np.mean(np.abs(dbp_ori_test - dbp_ori_pred_lstm))\n",
    "    rmse_dbp_lstm = np.sqrt(np.mean((dbp_ori_test - dbp_ori_pred_lstm)**2))\n",
    "    print(\"MAE:\", mae_sbp_lstm)\n",
    "    print(\"MAE:\", mae_dbp_lstm)\n",
    "    print(\"RMSE:\", rmse_sbp_lstm)\n",
    "    print(\"RMSE:\", rmse_dbp_lstm)\n",
    "    \n",
    "    print(i)\n",
    "    show_two(sbp_ori_test, sbp_ori_pred_lstm)\n",
    "    show_two(dbp_ori_test, dbp_ori_pred_lstm)\n",
    "    mae_sbp_lstms.append(mae_sbp_lstm)\n",
    "    rmse_sbp_lstms.append(rmse_sbp_lstm)\n",
    "    mae_dbp_lstms.append(mae_dbp_lstm)\n",
    "    rmse_dbp_lstms.append(rmse_dbp_lstm)\n",
    "    sbp_pred_gru = gru_model_sbp.predict(X_test)\n",
    "    sbp_ori_pred_gru = sbp_pred_gru * 250\n",
    "    mae_sbp_gru = np.mean(np.abs(sbp_ori_test - sbp_ori_pred_gru))\n",
    "    rmse_sbp_gru = np.sqrt(np.mean((sbp_ori_test - sbp_ori_pred_gru)**2))\n",
    "    dbp_pred_gru = gru_model_dbp.predict(X_test)\n",
    "    dbp_ori_pred_gru = dbp_pred_gru * 250\n",
    "    mae_dbp_gru = np.mean(np.abs(dbp_ori_test - dbp_ori_pred_gru))\n",
    "    rmse_dbp_gru = np.sqrt(np.mean((dbp_ori_test - dbp_ori_pred_gru)**2))\n",
    "    print(\"MAE:\", mae_sbp_gru)\n",
    "    print(\"MAE:\", mae_dbp_gru)\n",
    "    print(\"RMSE:\", rmse_sbp_gru)\n",
    "    print(\"RMSE:\", rmse_dbp_gru)\n",
    "    show_two(sbp_ori_test, sbp_ori_pred_gru)\n",
    "    show_two(dbp_ori_test, dbp_ori_pred_gru) \n",
    "    mae_sbp_grus.append(mae_sbp_gru)\n",
    "    rmse_sbp_grus.append(rmse_sbp_gru)\n",
    "    mae_dbp_grus.append(mae_dbp_gru)\n",
    "    rmse_dbp_grus.append(rmse_dbp_gru)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(mae_sbp_lstms), np.mean(rmse_sbp_lstms), np.mean(mae_dbp_lstms), np.mean(rmse_dbp_lstms))\n",
    "print(np.mean(mae_sbp_grus), np.mean(rmse_sbp_grus), np.mean(mae_dbp_grus), np.mean(rmse_dbp_grus))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
