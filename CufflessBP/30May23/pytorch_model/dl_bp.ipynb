{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the data\n",
    "import scipy.io\n",
    "data = scipy.io.loadmat('/Users/jinyanwei/Desktop/BP_Model/Data/Cuffless_BP_Estimation/part_1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs =125 # Sample rate in Hz\n",
    "\n",
    "import scipy.signal as signal\n",
    "def chebyshev_filter(signal):\n",
    "    # Define the filter order and cutoff frequency\n",
    "    order = 4\n",
    "    cutoff_freq = 20  # Cutoff frequency in Hz\n",
    "    # Create the Chebyshev low-pass filter\n",
    "    b, a = signal.cheby1(order, 0.5, cutoff_freq / (fs / 2), 'low', analog=False)\n",
    "    return signal.lfilter(b, a, signal)\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "def butter_filter(signal):\n",
    "    fs = 125  # Sample rate\n",
    "    cutoff = 5  # Cutoff frequency in Hz\n",
    "    # Design the Butterworth low-pass filter\n",
    "    nyquist = 0.5 * fs\n",
    "    cutoff_norm = cutoff / nyquist\n",
    "    b, a = butter(4, cutoff_norm, btype='low')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "import numpy as np\n",
    "def straighten_ecg(ecg_signal):\n",
    "    detrended_ecg = np.subtract(ecg_signal, np.mean(ecg_signal))\n",
    "    return detrended_ecg\n",
    "    \n",
    "import numpy as np\n",
    "def normalize_sinal(ppg):\n",
    "# Assuming ppg_signal and ecg_signal are your original PPG and ECG signals\n",
    "    ppg_min = np.min(ppg)\n",
    "    ppg_max = np.max(ppg)\n",
    "    normalized_ppg = (ppg - ppg_min) / (ppg_max - ppg_min)\n",
    "    return normalized_ppg\n",
    "def standard_signal(bp): \n",
    "    return (bp - np.mean(bp)) / np.std(bp)\n",
    "def inverse_standard_signal(bp_ori, bp_est):\n",
    "    mean = np.mean(bp_ori)\n",
    "    std = np.std(bp_ori)\n",
    "    return (bp_est * std) + mean\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "def align_ppgbp_segment(ppg_signal, bp_signal1, bp_signal2, ecg_signal, show=0): ## ppg_signal = ppg_normalized, bp_signal = bp_normalized, ecg_signal = ecg_normalized; get ppg_aligned, bp_aligned\n",
    "    ppg_peaks, _ = find_peaks(ppg_signal, height=0.5)  # Adjust the height threshold as needed\n",
    "    bp_peaks, _ = find_peaks(bp_signal1, height=0.4)\n",
    "    ecg_peaks, _ = find_peaks(ecg_signal, height=0.65)\n",
    "    #print(f'ppg peaks: {len(ppg_peaks)} {ppg_peaks}')\n",
    "    #print(f'ecg peaks: {len(ecg_peaks)} {ecg_peaks}')\n",
    "\n",
    "    first_ecg_peak = ecg_peaks[0]\n",
    "    #print(f'first ecg peak: {first_ecg_peak}')\n",
    "    indices_ppg = np.argwhere(ppg_peaks[:10] > first_ecg_peak)\n",
    "    first_ppg_peak = ppg_peaks[int(indices_ppg[0])]\n",
    "    #print(f'first ppg peak: {first_ppg_peak}')\n",
    "    indices_bp = np.argwhere(bp_peaks[:10] > first_ecg_peak)\n",
    "    first_bp_peak = bp_peaks[int(indices_bp[0])]\n",
    "    #print(f'first bp peak: {first_bp_peak}')\n",
    "    ppg_bp_peaks_subtraction = abs(bp_peaks[int(indices_bp[0]):int(indices_bp[0])+20] - ppg_peaks[int(indices_ppg[0]):int(indices_ppg[0])+20])\n",
    "    #print(ppg_bp_peaks_subtraction)\n",
    "    distance_ppgbp = np.bincount(ppg_bp_peaks_subtraction).argmax()\n",
    "    #print(move_distance)\n",
    "    #print(bp_peaks[int(indices_bp[0]):int(indices_bp[0])+20] - ppg_peaks[int(indices_ppg[0]):int(indices_ppg[0])+20])\n",
    "    #print(f'distance:{distance_ppgbp}')\n",
    "    if first_bp_peak > first_ppg_peak:\n",
    "        bp_aligned = bp_signal1[distance_ppgbp:]\n",
    "        bp_ori_aligned = bp_signal2[distance_ppgbp:]\n",
    "        ppg_aligned = ppg_signal\n",
    "    elif first_bp_peak < first_ppg_peak:\n",
    "        bp_aligned = bp_signal1\n",
    "        bp_ori_aligned = bp_signal2\n",
    "        ppg_aligned = ppg_signal[distance_ppgbp:]\n",
    "    else:\n",
    "        bp_aligned = bp_signal1\n",
    "        bp_ori_aligned = bp_signal2\n",
    "        ppg_aligned = ppg_signal\n",
    "    #print(f'ppg len: {len(ppg_aligned)}')\n",
    "    #print(f'bp len: {len(bp_aligned)}')\n",
    "    min_len = min(len(bp_aligned), len(ppg_aligned))\n",
    "    ppg_aligned = ppg_aligned[:min_len]\n",
    "    bp_aligned = bp_aligned[:min_len]\n",
    "    bp_ori_aligned = bp_ori_aligned[:min_len]\n",
    "    ecg_aligned = ecg_signal[:min_len]\n",
    "    #print(ecg_aligned)\n",
    "    ppg_segmented = ppg_aligned[:first_ecg_peak-5]\n",
    "    bp_segmented = bp_aligned[:first_ecg_peak-5]\n",
    "    bp_ori_segmented = bp_ori_aligned[:first_ecg_peak-5]\n",
    "    ecg_segmented = ecg_aligned[:first_ecg_peak-5]\n",
    "\n",
    "    for ecgi in range(len(ecg_peaks)-1):\n",
    "        one_ppg_peak, _ = find_peaks(ppg_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5], height=0.5)\n",
    "        #print(ecg_peaks[ecgi], one_ppg_peak)\n",
    "        if len(one_ppg_peak) == 1:\n",
    "            ppg_segmented = np.concatenate((ppg_segmented, ppg_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5]))\n",
    "            bp_segmented = np.concatenate((bp_segmented, bp_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5]))\n",
    "            bp_ori_segmented = np.concatenate((bp_ori_segmented, bp_ori_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5]))\n",
    "            ecg_segmented = np.concatenate((ecg_segmented, ecg_aligned[ecg_peaks[ecgi]-5:ecg_peaks[ecgi + 1]-5]))\n",
    "\n",
    "    if show == 1:\n",
    "        plt.figure(figsize=(30, 6))\n",
    "        plt.plot(ppg_signal, label='PPG')\n",
    "        plt.plot(bp_signal1, label='BP')\n",
    "        plt.plot(ecg_signal, label='ECG')\n",
    "        plt.scatter(ppg_peaks, ppg_signal[ppg_peaks], color='c', marker='o', label='Aligned PPG Peaks')\n",
    "        plt.scatter(bp_peaks, bp_signal1[bp_peaks], color='orange', marker='o', label='Aligned BP Peaks')\n",
    "        plt.scatter(ecg_peaks, ecg_signal[ecg_peaks], color='green', marker='o', label='Aligned BP Peaks')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(30, 6))\n",
    "        plt.plot(ppg_segmented, label='PPG')\n",
    "        plt.plot(bp_segmented, label='BP')\n",
    "        plt.plot(ecg_segmented, label='ECG')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Amplitude')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return ppg_segmented, bp_segmented, bp_ori_segmented, ecg_segmented\n",
    "\n",
    "import numpy as np\n",
    "def get_feautres(ppg_signal, bp_signal1, bp_signal2, ecg_signal):\n",
    "    ori_ecg_peaks, _ = find_peaks(ecg_signal)\n",
    "    r_peaks = np.array([ecg_peak for ecg_peak in ori_ecg_peaks if ecg_signal[ecg_peak] > 0.65])\n",
    "    #print(f'r_peaks: {len(r_peaks)} {r_peaks}')\n",
    "    r_peak_amplitudes = (ecg_signal[r_peaks]).tolist()\n",
    "    r_peak_amplitudes = r_peak_amplitudes[:-1]\n",
    "    #print(f'r_peak_amplitudes: {len(r_peak_amplitudes)} {r_peak_amplitudes}')\n",
    "    r_peak_intervals = (np.diff(r_peaks) / fs).tolist()\n",
    "    #print(f'r_peak_intervals: {len(r_peak_intervals)} {r_peak_intervals}')\n",
    "    # calculate low peak, s-peak\n",
    "    low_peak_amplitudes, r_to_low_peak_amplitudes, s_peak_amplitudes= [], [], []\n",
    "    low_peaks, s_peaks = [], []\n",
    "    for i in range(len(r_peaks) - 1):\n",
    "        r_peak = r_peaks[i]\n",
    "        next_r_peak = r_peaks[i + 1]\n",
    "        low_peak_amplitude = np.min(ecg_signal[r_peak:next_r_peak])\n",
    "        r_to_low_peak_amplitude = ecg_signal[r_peak]-low_peak_amplitude\n",
    "        low_peak_amplitudes.append(low_peak_amplitude)\n",
    "        r_to_low_peak_amplitudes.append(r_to_low_peak_amplitude)\n",
    "        low_peak = r_peak + np.argmin(ecg_signal[r_peak:next_r_peak])\n",
    "        low_peaks.append(low_peak)\n",
    "        s_peak_amplitude = np.min(ecg_signal[r_peak:low_peak])\n",
    "        s_peak_amplitudes.append(s_peak_amplitude)\n",
    "        s_peak = r_peak + np.argmin(ecg_signal[r_peak:low_peak])\n",
    "        s_peaks.append(s_peak)   \n",
    "    #print(f'low peaks: {len(low_peaks)} {low_peaks}')  \n",
    "    #print(f's peaks: {len(s_peaks)} {s_peaks}')  \n",
    "    # T-Wave Amplitude Calculation\n",
    "    r_peaks = np.insert(r_peaks, 0, 0) #the first t-peak is before the R-peak\n",
    "    t_wave_amplitudes, q_wave_amplitudes = [], []\n",
    "    t_peaks, q_peaks = [], []\n",
    "    for i in range(len(r_peaks) - 1):\n",
    "        r_peak = r_peaks[i]\n",
    "        next_r_peak = r_peaks[i + 1]\n",
    "        t_wave_amplitude = np.max(ecg_signal[r_peak:next_r_peak])\n",
    "        t_wave_amplitudes.append(t_wave_amplitude)\n",
    "        t_peak = r_peak + np.argmax(ecg_signal[r_peak:next_r_peak])\n",
    "        t_peaks.append(t_peak)\n",
    "        q_wave_amplitude = np.min(ecg_signal[t_peak:next_r_peak])\n",
    "        q_wave_amplitudes.append(q_wave_amplitude)\n",
    "        q_peak = r_peak + np.argmin(ecg_signal[t_peak:next_r_peak])\n",
    "        q_peaks.append(q_peak)\n",
    "\n",
    "    t_wave_amplitudes = t_wave_amplitudes[:-1]\n",
    "    q_wave_amplitudes = q_wave_amplitudes[:-1]\n",
    "    #print(f't peaks: {len(t_peaks)} {t_peaks}')  \n",
    "    #print(f'q peaks: {len(q_peaks)} {q_peaks}')  \n",
    "    \n",
    "    # QRS interval\n",
    "    r_peaks = np.array([ecg_peak for ecg_peak in ori_ecg_peaks if ecg_signal[ecg_peak] > 0.65])\n",
    "    qrs_intervals = []\n",
    "    for i in range(len(r_peaks)-1):\n",
    "        qrs_interval = (ecg_signal[s_peaks[i]] - ecg_signal[q_peaks[i]]) / fs\n",
    "        qrs_intervals.append(abs(qrs_interval))\n",
    "    #print(f'qrs_intervals: {len(qrs_intervals)} {qrs_intervals}')  \n",
    "\n",
    "    # get ppg features:\n",
    "    ppg_pulses, bp_pulses, bp_ori_pulses = [], [], []\n",
    "    for i in range(len(r_peaks)-1):\n",
    "        ppg_pulse = ppg_signal[r_peaks[i]:r_peaks[i+1]]\n",
    "        bp_pulse = bp_signal1[r_peaks[i]:r_peaks[i+1]]\n",
    "        bp_ori_pulse = bp_signal2[r_peaks[i]:r_peaks[i+1]]\n",
    "        ppg_pulses.append(ppg_pulse)\n",
    "        bp_pulses.append(bp_pulse)\n",
    "        bp_ori_pulses.append(bp_ori_pulse)\n",
    "    #print(f'bp pulses: {bp_pulses}')\n",
    "    ppg_pulse_amplitude = [np.max(pulse) - np.min(pulse) for pulse in ppg_pulses]\n",
    "    ppg_pulse_width = [pulse.shape[0]/fs for pulse in ppg_pulses]\n",
    "    ppg_high_to_low_interval = [np.argmin(pulse)/fs for pulse in ppg_pulses]\n",
    "    ppg_slope_change_std = [np.std(np.diff(pulse)) for pulse in ppg_pulses]\n",
    "\n",
    "    sbp = [np.max(pulse) for pulse in bp_pulses]\n",
    "    dbp = [np.min(pulse) for pulse in bp_pulses]\n",
    "    sbp_ori = [np.max(pulse) for pulse in bp_ori_pulses]\n",
    "    dbp_ori = [np.min(pulse) for pulse in bp_ori_pulses]\n",
    "    \n",
    "    return sbp, dbp, sbp_ori, dbp_ori, ppg_pulse_amplitude, ppg_pulse_width, ppg_high_to_low_interval, ppg_slope_change_std, t_wave_amplitudes, q_wave_amplitudes, r_peak_amplitudes, s_peak_amplitudes, low_peak_amplitudes, qrs_intervals, r_to_low_peak_amplitudes, r_peak_intervals\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show_one(signal1):\n",
    "    fig = plt.figure(figsize=(30,6))\n",
    "    plt.plot(signal1)\n",
    "    return plt.show()\n",
    "def show_two(signal1, signal2):\n",
    "    fig = plt.figure(figsize=(30,6))\n",
    "    plt.plot(signal1, label='1')\n",
    "    plt.plot(signal2, label='2')\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "def show_three(signal1, signal2, signal3):\n",
    "    fig = plt.figure(figsize=(30,6))\n",
    "    plt.plot(signal1, label='1')\n",
    "    plt.plot(signal2, label='2')\n",
    "    plt.plot(signal3, label='3')\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, (12, 804), (804,), (804,), (804,), (804,), (12, 300), (300,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Can't be used data: patient_unable = []\n",
    "## Some wrongs data: patient_better_dont_ues = [2]\n",
    "## data['p'].shape:(1, 1000); patient_data = data['p'][0,patient_num], shape:(3, points)\n",
    "patient_data = data['p'][0,18]\n",
    "ppg_ori = patient_data[0]\n",
    "bp_ori = patient_data[1]\n",
    "ecg_ori = patient_data[2]\n",
    "\n",
    "ecg_detrened = straighten_ecg(ecg_ori)\n",
    "ppg_normalized = normalize_sinal(ppg_ori)\n",
    "bp_standarded = bp_ori / 250\n",
    "ecg_normalized = normalize_sinal(ecg_detrened)\n",
    "ppg_segmented, bp_segmented, bp_ori_segmented, ecg_segmented = align_ppgbp_segment(ppg_signal = ppg_normalized, bp_signal1 = bp_standarded, bp_signal2 = bp_ori, ecg_signal = ecg_normalized, show=0)\n",
    "bps_features = get_feautres(ppg_segmented, bp_segmented, bp_ori_segmented, ecg_segmented)\n",
    "sbp = np.array(bps_features[0])\n",
    "dbp = np.array(bps_features[1])\n",
    "sbp_ori = np.array(bps_features[2])\n",
    "dbp_ori = np.array(bps_features[3])\n",
    "features = np.array(bps_features[4:])\n",
    "np.max(features), features.shape, sbp.shape, dbp.shape,sbp_ori.shape, dbp_ori.shape,features[:, :300].shape, sbp[:300].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = []\n",
    "for patient in range(1000):\n",
    "    patient_data = data['p'][0,patient]\n",
    "    if len(patient_data[0]) >= 25000:\n",
    "        patients.append(patient)\n",
    "print(len(patients), patients)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = [0, 1, 2, 3, 4, 6, 14, 15, 16, 18, 24, 35, 36, 37, 39, 40, 41, 46, 64, 66, 67, 73, 79, 80, 81, 82, 86, 87, 88, 89, 93, 94, 95, 98, 99, 100, 101, 102, 103, 104, 105, 108, 109, 110, 111, 115, 116, 117, 118, 120, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 141, 144, 149, 150, 151, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 179, 182, 184, 186, 188, 189, 192, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 206, 207, 208, 209, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 245, 246, 247, 248, 249, 251, 252, 253, 254, 255, 256, 257, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 272, 273, 277, 278, 279, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 295, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 309, 310, 311, 313, 314, 315, 316, 317, 318, 319, 320, 321, 323, 324, 327, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 377, 380, 391, 393, 394, 395, 397, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 413, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 429, 430, 431, 432, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 459, 460, 462, 463, 464, 465, 466, 470, 471, 472, 475, 480, 481, 482, 483, 484, 486, 487, 488, 490, 491, 492, 496, 497, 498, 499, 500, 502, 503, 504, 505, 507, 508, 509, 510, 512, 514, 515, 516, 520, 524, 527, 528, 529, 530, 531, 532, 533, 535, 537, 538, 539, 543, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 564, 565, 570, 576, 577, 578, 582, 590, 592, 593, 597, 599, 600, 602, 603, 604, 606, 607, 609, 612, 625, 626, 627, 628, 629, 630, 633, 634, 635, 639, 640, 641, 644, 646, 648, 653, 655, 656, 657, 658, 662, 670, 673, 674, 676, 677, 681, 682, 683, 685, 686, 687, 688, 689, 692, 693, 695, 696, 697, 698, 700, 703, 704, 705, 706, 707, 708, 709, 711, 712, 717, 719, 720, 721, 722, 723, 724, 725, 727, 729, 730, 731, 733, 734, 735, 736, 737, 741, 743, 744, 748, 749, 753, 754, 755, 756, 759, 774, 775, 776, 777, 780, 781, 783, 784, 785, 786, 787, 790, 791, 792, 794, 799, 800, 801, 802, 803, 805, 807, 808, 809, 810, 813, 820, 821, 822, 823, 828, 829, 830, 843, 845, 847, 868, 869, 922, 923, 947, 948, 949, 951, 952, 957, 959, 960, 962, 972, 975, 981, 989]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all patient dataset\n",
    "sbp500 = []\n",
    "dbp500 = []\n",
    "sbp_ori500 = []\n",
    "dbp_ori500 = []\n",
    "features500 = []\n",
    "for patient in patients:\n",
    "    patient_data = data['p'][0,patient]\n",
    "    if len(patient_data[0]) >= 25000:\n",
    "        ppg_ori = patient_data[0]\n",
    "        bp_ori = patient_data[1]\n",
    "        ecg_ori = patient_data[2]\n",
    "        ecg_detrened = straighten_ecg(ecg_ori)\n",
    "        ppg_normalized = normalize_sinal(ppg_ori)\n",
    "        bp_standarded = bp_ori / 250\n",
    "        ecg_normalized = normalize_sinal(ecg_detrened)\n",
    "        ppg_segmented, bp_segmented, bp_ori_segmented, ecg_segmented = align_ppgbp_segment(ppg_signal = ppg_normalized, bp_signal1 = bp_standarded, bp_signal2 = bp_ori, ecg_signal = ecg_normalized, show=0)\n",
    "        bps_features = get_feautres(ppg_segmented, bp_segmented, bp_ori_segmented, ecg_segmented)\n",
    "        sbp = np.array(bps_features[0])\n",
    "        dbp = np.array(bps_features[1])\n",
    "        sbp_ori = np.array(bps_features[2])\n",
    "        dbp_ori = np.array(bps_features[3])\n",
    "        features = np.array(bps_features[4:])\n",
    "        sbp500.append(sbp)\n",
    "        dbp500.append(dbp)\n",
    "        sbp_ori500.append(sbp_ori)\n",
    "        dbp_ori500.append(dbp_ori)\n",
    "        features500.append(features)\n",
    "        print(patient)\n",
    "len(sbp_ori500)    \n",
    "features500 = np.array(features500)\n",
    "np.save('sbp500.npy', sbp500)\n",
    "np.save('sbp_ori500.npy', sbp_ori500)\n",
    "np.save('dbp500.npy', dbp500)\n",
    "np.save('dbp_ori500.npy', dbp_ori500)\n",
    "np.save('features500.npy', features500)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sbp500 = np.load('sbp500.npy')\n",
    "sbp_ori500 = np.load('sbp_ori500.npy')\n",
    "dbp500 = np.load('dbp500.npy')\n",
    "dbp_ori500 = np.load('dbp_ori500.npy')\n",
    "features500 = np.load('features500.npy').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 804)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split train and test dataset\n",
    "train_sbp = sbp500[:200]\n",
    "train_dbp = dbp500[:200]\n",
    "train_sbp_ori = sbp_ori500[:200]\n",
    "train_dbp_ori = dbp_ori500[:200]\n",
    "train_features = features500[:200]\n",
    "test_sbp = []\n",
    "test_dbp = []\n",
    "test_sbp_ori = []\n",
    "test_dbp_ori = []\n",
    "test_features = []\n",
    "\n",
    "for i in range(len(sbp500)-400):\n",
    "    train_sbp.append(sbp500[i+400][:int(0.2*len(sbp500[i+400]))])\n",
    "    train_dbp.append(dbp500[i+400][:int(0.2*len(dbp500[i+400]))])\n",
    "    train_sbp_ori.append(sbp_ori500[i+400][:int(0.2*len(sbp_ori500[i+400]))])\n",
    "    train_dbp_ori.append(dbp_ori500[i+400][:int(0.2*len(dbp_ori500[i+400]))])\n",
    "    train_features.append(features500[i+400][:,:int(0.2*(features500[i+400].shape[1]))])\n",
    "    test_sbp.append(sbp500[i+400][int(0.2*len(sbp500[i+400])):])\n",
    "    test_dbp.append(dbp500[i+400][int(0.2*len(dbp500[i+400])):])\n",
    "    test_sbp_ori.append(sbp_ori500[i+400][int(0.2*len(sbp_ori500[i+400])):])\n",
    "    test_dbp_ori.append(dbp_ori500[i+400][int(0.2*len(dbp_ori500[i+400])):])\n",
    "    test_features.append(features500[i+400][:,int(0.2*(features500[i+400].shape[1])):])\n",
    "\n",
    "merged_train_sbp = np.concatenate(train_sbp, axis=0)\n",
    "merged_train_dbp = np.concatenate(train_dbp, axis=0)\n",
    "merged_train_sbp_ori = np.concatenate(train_sbp_ori, axis=0)\n",
    "merged_train_dbp_ori = np.concatenate(train_dbp_ori, axis=0)\n",
    "merged_train_features = np.concatenate(train_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## array X_train, linear and forest model\n",
    "X_train = merged_train_features.T\n",
    "sbp_train = merged_train_sbp\n",
    "dbp_train = merged_train_dbp\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "linear_model_sbp = LinearRegression()\n",
    "linear_model_sbp.fit(X_train, sbp_train)\n",
    "linear_model_dbp = LinearRegression()\n",
    "linear_model_dbp.fit(X_train, dbp_train)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "X_train = X_train.reshape(-1, 1) if len(X_train.shape) == 1 else X_train\n",
    "randomforest_model_sbp = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "randomforest_model_sbp.fit(X_train, sbp_train)\n",
    "randomforest_model_dbp = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "randomforest_model_dbp.fit(X_train, dbp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test, linear and forest result\n",
    "mae_sbp_linears = []\n",
    "rmse_sbp_linears = []\n",
    "mae_sbp_randomforests = []\n",
    "rmse_sbp_randomforests = []\n",
    "mae_dbp_linears = []\n",
    "rmse_dbp_linears = []\n",
    "mae_dbp_randomforests = []\n",
    "rmse_dbp_randomforests = []\n",
    "for i in range(1):#len(test_sbp)\n",
    "    X_test = test_features[i].T\n",
    "    sbp_test = test_sbp[i]\n",
    "    sbp_test_ori = test_sbp_ori[i]\n",
    "    dbp_test = test_dbp[i]\n",
    "    dbp_test_ori = test_dbp_ori[i]\n",
    "    sbp_pred_linear = linear_model_sbp.predict(X_test)\n",
    "    sbp_pred_ori_linear = sbp_pred_linear * 250\n",
    "    dbp_pred_linear = linear_model_dbp.predict(X_test)\n",
    "    dbp_pred_ori_linear = dbp_pred_linear * 250\n",
    "    # Evaluate the model\n",
    "    mae_sbp_linear = mean_absolute_error(sbp_test_ori, sbp_pred_ori_linear)\n",
    "    rmse_sbp_linear = mean_squared_error(sbp_test_ori, sbp_pred_ori_linear, squared=False)\n",
    "    mae_dbp_linear = mean_absolute_error(dbp_test_ori, dbp_pred_ori_linear)\n",
    "    rmse_dbp_linear = mean_squared_error(dbp_test_ori, dbp_pred_ori_linear, squared=False)\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Mean Squared Error (MSE):\", rmse_sbp_linear)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae_dbp_linear)\n",
    "    show_two(sbp_test, sbp_pred_linear)\n",
    "    show_two(dbp_test_ori, dbp_pred_ori_linear)\n",
    "\n",
    "    X_test = X_test.reshape(-1, 1) if len(X_test.shape) == 1 else X_test\n",
    "    sbp_pred_randomforest = randomforest_model_sbp.predict(X_test)\n",
    "    sbp_pred_ori_randomforest = sbp_pred_randomforest * 250   \n",
    "    dbp_pred_randomforest = randomforest_model_dbp.predict(X_test)\n",
    "    dbp_pred_ori_randomforest = dbp_pred_randomforest * 250\n",
    "    rmse_sbp_randomforest = mean_squared_error(sbp_test_ori, sbp_pred_ori_randomforest, squared=False)\n",
    "    mae_sbp_randomforest = mean_absolute_error(sbp_test_ori, sbp_pred_ori_randomforest)\n",
    "    rmse_dbp_randomforest = mean_squared_error(dbp_test_ori, dbp_pred_ori_randomforest, squared=False)\n",
    "    mae_dbp_randomforest = mean_absolute_error(dbp_test_ori, dbp_pred_ori_randomforest)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse_sbp_randomforest)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae_dbp_randomforest)\n",
    "    show_two(sbp_test, sbp_pred_randomforest)\n",
    "    show_two(dbp_test_ori, dbp_pred_ori_randomforest)\n",
    "    mae_sbp_linears.append(mae_sbp_linear)\n",
    "    rmse_sbp_linears.append(rmse_sbp_linear)\n",
    "    mae_sbp_randomforests.append(mae_sbp_randomforest)\n",
    "    rmse_sbp_randomforests.append(rmse_sbp_randomforest)\n",
    "    mae_dbp_linears.append(mae_dbp_linear)\n",
    "    rmse_dbp_linears.append(rmse_dbp_linear)\n",
    "    mae_dbp_randomforests.append(mae_dbp_randomforest)\n",
    "    rmse_dbp_randomforests.append(rmse_dbp_randomforest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensor X_train\n",
    "import torch\n",
    "time_step = 4\n",
    "X_train_noshape = torch.tensor(merged_train_features)\n",
    "sbp_train_noshape = torch.tensor(merged_train_sbp)\n",
    "dbp_train_noshape = torch.tensor(merged_train_dbp)\n",
    "\n",
    "train_num_samples = X_train_noshape.shape[1] - time_step + 1\n",
    "X_train = torch.zeros((train_num_samples, time_step, X_train_noshape.shape[0]))\n",
    "sbp_train = torch.zeros((train_num_samples, 1))\n",
    "dbp_train = torch.zeros((train_num_samples, 1))\n",
    "for i in range(train_num_samples):\n",
    "    X_train[i] = X_train_noshape[:, i:i+time_step].T\n",
    "    sbp_train[i] = sbp_train_noshape[i+time_step-1]\n",
    "    dbp_train[i] = dbp_train_noshape[i+time_step-1]\n",
    "\n",
    "print(X_train.shape, sbp_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        output = self.linear(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "lstm_model_sbp = LSTMModel(input_size = 12, hidden_size = 64, output_size = 1)\n",
    "lstm_model_dbp = LSTMModel(input_size = 12, hidden_size = 64, output_size = 1)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_model_sbp.parameters(), lr=0.001)\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs_sbp = lstm_model_sbp(X_train)   \n",
    "    loss_sbp = criterion(outputs_sbp, sbp_train)\n",
    "    loss_sbp.backward()\n",
    "    outputs_dbp = lstm_model_sbp(X_train)   \n",
    "    loss_dbp = criterion(outputs_dbp, dbp_train)\n",
    "    loss_dbp.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss_sbp.item()}, Loss: {loss_dbp.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM-GRU model\n",
    "class LSTMGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMGRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states for LSTM\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        # GRU forward pass\n",
    "        gru_out, _ = self.gru(lstm_out)\n",
    "        # Get the final output from the last time step of GRU\n",
    "        output = gru_out[:, -1, :]\n",
    "        # Fully connected layer\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "lstmgru_model_sbp = LSTMGRUModel(input_size =12, hidden_size=64, num_layers=2, output_size=1)\n",
    "lstmgru_model_dbp = LSTMGRUModel(input_size =12, hidden_size=64, num_layers=2, output_size=1)\n",
    "# Define the loss function and optimizer\n",
    "criterion_sbp = nn.MSELoss()\n",
    "optimizer_sbp = torch.optim.Adam(lstmgru_model_sbp.parameters(), lr=0.001)\n",
    "num_epochs = 2600\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    lstmgru_model_sbp.train()\n",
    "    optimizer_sbp.zero_grad()\n",
    "    sbp_outputs = lstmgru_model_sbp(X_train)\n",
    "    loss_sbp = criterion_sbp(sbp_outputs, sbp_train)\n",
    "    loss_sbp.backward()\n",
    "    optimizer_sbp.step()\n",
    "criterion_dbp = nn.MSELoss()\n",
    "optimizer_dbp = torch.optim.Adam(lstmgru_model_dbp.parameters(), lr=0.001)\n",
    "num_epochs = 2600\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    lstmgru_model_dbp.train()\n",
    "    optimizer_dbp.zero_grad()\n",
    "    dbp_outputs = lstmgru_model_dbp(X_train)\n",
    "    loss_dbp = criterion_dbp(dbp_outputs, dbp_train)\n",
    "    loss_dbp.backward()\n",
    "    optimizer_dbp.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):#len(test_sbp)\n",
    "    X_test_noshape = torch.tensor(test_features[i])\n",
    "    sbp_test_noshape = torch.tensor(test_sbp[i])\n",
    "    sbp_test_ori_noshape = torch.tensor(test_sbp_ori[i])\n",
    "    dbp_test_noshape = torch.tensor(test_dbp[i])\n",
    "    dbp_test_ori_noshape = torch.tensor(test_dbp_ori[i])\n",
    "    test_num_samples = X_test_noshape.shape[1] - time_step + 1\n",
    "    X_test = torch.zeros((test_num_samples, time_step, X_test_noshape.shape[0]))\n",
    "    sbp_test = torch.zeros((test_num_samples, 1))\n",
    "    sbp_test_ori = torch.zeros((test_num_samples, 1))\n",
    "    dbp_test = torch.zeros((test_num_samples, 1))\n",
    "    dbp_test_ori = torch.zeros((test_num_samples, 1))\n",
    "    for i in range(test_num_samples):\n",
    "        X_test[i] = X_test_noshape[:, i:i+time_step].T\n",
    "        sbp_test[i] = sbp_test_noshape[i+time_step-1]\n",
    "        sbp_test_ori[i] = sbp_test_ori_noshape[i+time_step-1]\n",
    "        dbp_test[i] = dbp_test_noshape[i+time_step-1]\n",
    "        dbp_test_ori[i] = dbp_test_ori_noshape[i+time_step-1]\n",
    "    \n",
    "    lstm_model_sbp.eval()\n",
    "    with torch.no_grad():\n",
    "        sbp_pred_lstm = lstm_model_sbp(X_test)\n",
    "        sbp_pred_ori_lstm = sbp_pred_lstm * 250\n",
    "        test_loss_sbp_lstm = criterion(sbp_pred_lstm, sbp_test)\n",
    "        print(f\"Test Loss: {test_loss_sbp_lstm.item()}\")\n",
    "    mae_sbp_lstm = mean_absolute_error(np.array(sbp_test_ori), np.array(sbp_pred_ori_lstm))\n",
    "    rmse_sbp_lstm = np.sqrt(mean_squared_error(np.array(sbp_test_ori), np.array(sbp_pred_ori_lstm)))\n",
    "    lstm_model_dbp.eval()\n",
    "    with torch.no_grad():\n",
    "        dbp_pred_lstm = lstm_model_dbp(X_test)\n",
    "        dbp_pred_ori_lstm = dbp_pred_lstm * 250\n",
    "        test_loss_dbp_lstm = criterion(dbp_pred_lstm, dbp_test)\n",
    "        print(f\"Test Loss: {test_loss_dbp_lstm.item()}\")\n",
    "    mae_dbp_lstm = mean_absolute_error(np.array(dbp_test_ori), np.array(dbp_pred_ori_lstm))\n",
    "    rmse_dbp_lstm = np.sqrt(mean_squared_error(np.array(dbp_test_ori), np.array(dbp_pred_ori_lstm)))\n",
    "    print(\"MAE:\", mae_dbp_lstm)\n",
    "    print(\"RMSE:\", rmse_sbp_lstm)\n",
    "    show_two(sbp_test, sbp_pred_lstm)\n",
    "    show_two(dbp_test_ori, dbp_pred_ori_lstm)\n",
    "    # Evaluation lstmgru\n",
    "    lstmgru_model_sbp.eval()\n",
    "    with torch.no_grad():\n",
    "        sbp_pred_lstmgru = lstmgru_model_sbp(X_test)\n",
    "        sbp_pred_ori_lstmgru = sbp_pred_lstmgru * 250\n",
    "        test_loss_sbp_lstmgru = criterion_sbp(sbp_pred_lstmgru, sbp_test)\n",
    "        print(f\"Test Loss: {test_loss_sbp_lstmgru.item()}\")\n",
    "    mae_sbp_lstmgru = mean_absolute_error(np.array(sbp_test_ori), np.array(sbp_pred_ori_lstmgru))\n",
    "    rmse_sbp_lstmgru = np.sqrt(mean_squared_error(np.array(sbp_test_ori), np.array(sbp_pred_ori_lstmgru)))\n",
    "    lstmgru_model_dbp.eval()\n",
    "    with torch.no_grad():\n",
    "        dbp_pred_lstmgru = lstmgru_model_dbp(X_test)\n",
    "        dbp_pred_ori_lstmgru = dbp_pred_lstmgru * 250\n",
    "        test_loss_dbp_lstmgru = criterion_dbp(dbp_pred_lstmgru, dbp_test)\n",
    "        print(f\"Test Loss: {test_loss_dbp_lstmgru.item()}\")\n",
    "    mae_dbp_lstmgru = mean_absolute_error(np.array(dbp_test_ori), np.array(dbp_pred_ori_lstmgru))\n",
    "    rmse_dbp_lstmgru = np.sqrt(mean_squared_error(np.array(dbp_test_ori), np.array(dbp_pred_ori_lstmgru)))\n",
    "    print(\"MAE:\", mae_sbp_lstmgru)\n",
    "    print(\"RMSE:\", rmse_dbp_lstmgru)\n",
    "    show_two(sbp_test, sbp_pred_lstmgru)\n",
    "    show_two(dbp_test_ori, dbp_pred_ori_lstmgru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "def linearregressionmodel():\n",
    "    # Assuming you have the following data:\n",
    "    # X_train: training data with shape (n_train_samples, n_features)\n",
    "    # y_train: target labels for the training data with shape (n_train_samples,)\n",
    "    # X_test: test data with shape (n_test_samples, n_features)\n",
    "    # y_test: target labels for the test data with shape (n_test_samples,)\n",
    "    features_linear = features.T\n",
    "    X_train = features_linear[:600]\n",
    "    y_train = sbp[:600]\n",
    "    X_test = features_linear[600:]\n",
    "    y_test = sbp[600:]\n",
    "    y_test_ori = sbp_ori[600:]\n",
    "\n",
    "    # Create a linear regression model\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_ori = y_pred * 250\n",
    "    # Evaluate the model\n",
    "    mae = mean_absolute_error(y_test_ori, y_pred_ori)\n",
    "    rmse = mean_squared_error(y_test_ori, y_pred_ori, squared=False)\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Mean Squared Error (MSE):\", rmse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    show_two(y_test, y_pred)\n",
    "    show_two(y_test_ori, y_pred_ori)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "def randomforestmodel():\n",
    "    X = features.T  # Shape: (n_samples, n_features)\n",
    "    y = sbp  # Shape: (n_samples,)\n",
    "    y_ori = sbp_ori\n",
    "    train_size = int(0.8 * len(X))  # Set your desired train-test split ratio\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    y_train_ori, y_test_ori = y_ori[:train_size], y_ori[train_size:]\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    X_train = X_train.reshape(-1, 1) if len(X_train.shape) == 1 else X_train\n",
    "    X_test = X_test.reshape(-1, 1) if len(X_test.shape) == 1 else X_test\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_ori = y_pred * 250\n",
    "    rmse = mean_squared_error(y_test_ori, y_pred_ori, squared=False)\n",
    "    mae = mean_absolute_error(y_test_ori, y_pred_ori)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    show_two(y_test, y_pred)\n",
    "    show_two(y_test_ori, y_pred_ori)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lstm pytorch\n",
    "# reshape the data\n",
    "import torch\n",
    "def lstmmodel():\n",
    "    time_step = 4\n",
    "    X_train_noshape = torch.tensor(features[:, :750])\n",
    "    y_train_noshape = torch.tensor(sbp[:750])\n",
    "    X_test_noshape = torch.tensor(features[:, 750:])\n",
    "    y_test_noshape = torch.tensor(sbp[750:])\n",
    "    y_test_ori_noshape = torch.tensor(sbp_ori[750:])\n",
    "    train_num_samples = X_train_noshape.shape[1] - time_step + 1\n",
    "    X_train = torch.zeros((train_num_samples, time_step, X_train_noshape.shape[0]))\n",
    "    y_train = torch.zeros((train_num_samples, 1))\n",
    "    for i in range(train_num_samples):\n",
    "        X_train[i] = X_train_noshape[:, i:i+time_step].T\n",
    "        y_train[i] = y_train_noshape[i+time_step-1]\n",
    "    test_num_samples = X_test_noshape.shape[1] - time_step + 1\n",
    "    X_test = torch.zeros((test_num_samples, time_step, X_test_noshape.shape[0]))\n",
    "    y_test = torch.zeros((test_num_samples, 1))\n",
    "    y_test_ori = torch.zeros((test_num_samples, 1))\n",
    "    for i in range(test_num_samples):\n",
    "        X_test[i] = X_test_noshape[:, i:i+time_step].T\n",
    "        y_test[i] = y_test_noshape[i+time_step-1]\n",
    "        y_test_ori[i] = y_test_ori_noshape[i+time_step-1]\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, y_test_ori.shape)\n",
    "\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    # Define the LSTM model\n",
    "    class LSTMModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(LSTMModel, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "            self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            output = self.linear(lstm_out[:, -1, :])\n",
    "            return output\n",
    "\n",
    "    # Create an instance of the LSTM model\n",
    "    model = LSTMModel(input_size = 12, hidden_size = 64, output_size = 1)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Training loop\n",
    "    num_epochs = 200\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)   \n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        y_pred_ori = y_pred * 250\n",
    "        test_loss = criterion(y_pred, y_test)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "    mae = mean_absolute_error(np.array(y_test_ori), np.array(y_pred_ori))\n",
    "    rmse = np.sqrt(mean_squared_error(np.array(y_test_ori), np.array(y_pred_ori)))\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    show_two(y_test, y_pred)\n",
    "    show_two(y_test_ori, y_pred_ori)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU model pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "def grumodel():\n",
    "    time_step = 4\n",
    "    X_train_noshape = torch.tensor(features[:, :600])\n",
    "    y_train_noshape = torch.tensor(sbp[:600])\n",
    "    X_test_noshape = torch.tensor(features[:, 600:])\n",
    "    y_test_noshape = torch.tensor(sbp[600:])\n",
    "    y_test_ori_noshape = torch.tensor(sbp_ori[600:])\n",
    "    train_num_samples = X_train_noshape.shape[1] - time_step + 1\n",
    "    X_train = torch.zeros((train_num_samples, time_step, X_train_noshape.shape[0]))\n",
    "    y_train = torch.zeros((train_num_samples, 1))\n",
    "    for i in range(train_num_samples):\n",
    "        X_train[i] = X_train_noshape[:, i:i+time_step].T\n",
    "        y_train[i] = y_train_noshape[i+time_step-1]\n",
    "    test_num_samples = X_test_noshape.shape[1] - time_step + 1\n",
    "    X_test = torch.zeros((test_num_samples, time_step, X_test_noshape.shape[0]))\n",
    "    y_test = torch.zeros((test_num_samples, 1))\n",
    "    y_test_ori = torch.zeros((test_num_samples, 1))\n",
    "    for i in range(test_num_samples):\n",
    "        X_test[i] = X_test_noshape[:, i:i+time_step].T\n",
    "        y_test[i] = y_test_noshape[i+time_step-1]\n",
    "        y_test_ori[i] = y_test_ori_noshape[i+time_step-1]\n",
    "    # Define the GRU model\n",
    "    class GRUModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "            super(GRUModel, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            out, _ = self.gru(x, h0)\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "    # Set the input size, hidden size, number of layers, and output size\n",
    "    # The number of input features, Number of units in the hidden layer, Number of GRU layers, Output size (SBP)\n",
    "    # Create an instance of the GRU model\n",
    "    model = GRUModel(input_size =12, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "    # Print the model architecture\n",
    "    print(model)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 100\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        y_pred_ori = y_pred * 250\n",
    "        test_loss = criterion(y_pred, y_test)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "    mae = mean_absolute_error(np.array(y_test_ori), np.array(y_pred_ori))\n",
    "    rmse = np.sqrt(mean_squared_error(np.array(y_test_ori), np.array(y_pred_ori)))\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    show_two(y_test, y_pred)\n",
    "    show_two(y_test_ori, y_pred_ori)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRU SVR model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.svm import SVR\n",
    "def grusvrmodel():\n",
    "    time_step = 4\n",
    "    features_linear = features.T\n",
    "    X_train = features_linear[:600]\n",
    "    y_train = sbp[:600]\n",
    "    X_test = features_linear[600:]\n",
    "    y_test = sbp[600:]\n",
    "    y_test_ori = sbp_ori[600:]\n",
    "    # Train the SVM model\n",
    "    svm_regressor = SVR(kernel='rbf')\n",
    "    svm_regressor.fit(X_train, y_train)\n",
    "    svm_predictions = svm_regressor.predict(X_test)\n",
    "    # Convert svm_predictions to a PyTorch tensor\n",
    "    svm_predictions_tensor = torch.tensor(svm_predictions)\n",
    "    # Convert X_test to a PyTorch tensor\n",
    "    X_test_tensor = torch.from_numpy(X_test)\n",
    "    # Reshape svm_predictions_tensor to match the shape of X_test_tensor\n",
    "    svm_predictions_reshaped = svm_predictions_tensor.reshape(-1, 1)\n",
    "    # Expand dimensions of svm_predictions_reshaped\n",
    "    svm_predictions_expanded = svm_predictions_reshaped.unsqueeze(2)\n",
    "    # Transpose svm_predictions_expanded\n",
    "    svm_predictions_transposed = svm_predictions_expanded.transpose(0, 1)\n",
    "    # Prepare input for the GRU model\n",
    "    combined_features = torch.cat((X_test_tensor, svm_predictions_transposed), dim=1)\n",
    "    # Define the GRU model\n",
    "    class GRUModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(GRUModel, self).__init__()\n",
    "            self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            _, h = self.gru(x)\n",
    "            output = self.fc(h)\n",
    "            return output\n",
    "    # Define model parameters\n",
    "    input_size = 12\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    output_size = 1\n",
    "    # Create the GRU model\n",
    "    gru_model = GRUModel(input_size=input_size + 1, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(gru_model.parameters(), lr=0.001)\n",
    "    # Convert data to torch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "    # Train the GRU model\n",
    "    num_epochs = 100\n",
    "    batch_size = 32\n",
    "    for epoch in range(num_epochs):\n",
    "        permutation = torch.randperm(X_train_tensor.size()[0])\n",
    "        for i in range(0, X_train_tensor.size()[0], batch_size):\n",
    "            indices = permutation[i:i + batch_size]\n",
    "            batch_X, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = gru_model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    # Make predictions using the trained GRU model\n",
    "    gru_model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = gru_model(combined_features)\n",
    "        y_pred_ori = y_pred * 250\n",
    "        test_loss = criterion(y_pred, y_test)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "    mae = mean_absolute_error(np.array(y_test_ori), np.array(y_pred_ori))\n",
    "    rmse = np.sqrt(mean_squared_error(np.array(y_test_ori), np.array(y_pred_ori)))\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    show_two(y_test, y_pred)\n",
    "    show_two(y_test_ori, y_pred_ori)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lstm-gru pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "def lstmgru_model():\n",
    "    time_step = 4\n",
    "    X_train_noshape = torch.tensor(features[:, :600])\n",
    "    y_train_noshape = torch.tensor(sbp[:600])\n",
    "    X_test_noshape = torch.tensor(features[:, 600:])\n",
    "    y_test_noshape = torch.tensor(sbp[600:])\n",
    "    y_test_ori_noshape = torch.tensor(sbp_ori[600:])\n",
    "    train_num_samples = X_train_noshape.shape[1] - time_step + 1\n",
    "    X_train = torch.zeros((train_num_samples, time_step, X_train_noshape.shape[0]))\n",
    "    y_train = torch.zeros((train_num_samples, 1))\n",
    "    for i in range(train_num_samples):\n",
    "        X_train[i] = X_train_noshape[:, i:i+time_step].T\n",
    "        y_train[i] = y_train_noshape[i+time_step-1]\n",
    "    test_num_samples = X_test_noshape.shape[1] - time_step + 1\n",
    "    X_test = torch.zeros((test_num_samples, time_step, X_test_noshape.shape[0]))\n",
    "    y_test = torch.zeros((test_num_samples, 1))\n",
    "    y_test_ori = torch.zeros((test_num_samples, 1))\n",
    "    for i in range(test_num_samples):\n",
    "        X_test[i] = X_test_noshape[:, i:i+time_step].T\n",
    "        y_test[i] = y_test_noshape[i+time_step-1]\n",
    "        y_test_ori[i] = y_test_ori_noshape[i+time_step-1]\n",
    "    class LSTMGRUModel(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "            super(LSTMGRUModel, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "\n",
    "            # LSTM layer\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "            # GRU layer\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "            # Fully connected layer\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Set initial hidden and cell states for LSTM\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "            # LSTM forward pass\n",
    "            lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "            # GRU forward pass\n",
    "            gru_out, _ = self.gru(lstm_out)\n",
    "\n",
    "            # Get the final output from the last time step of GRU\n",
    "            output = gru_out[:, -1, :]\n",
    "\n",
    "            # Fully connected layer\n",
    "            output = self.fc(output)\n",
    "\n",
    "            return output\n",
    "    model = LSTMGRUModel(input_size =12, hidden_size=64, num_layers=2, output_size=1)\n",
    "\n",
    "    # Print the model architecture\n",
    "    print(model)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 2600\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "        y_pred_ori = y_pred * 250\n",
    "        test_loss = criterion(y_pred, y_test)\n",
    "        print(f\"Test Loss: {test_loss.item()}\")\n",
    "    mae = mean_absolute_error(np.array(y_test_ori), np.array(y_pred_ori))\n",
    "    rmse = np.sqrt(mean_squared_error(np.array(y_test_ori), np.array(y_pred_ori)))\n",
    "    print(\"MAE:\", mae)\n",
    "    print(\"RMSE:\", rmse)\n",
    "    show_two(y_test, y_pred)\n",
    "    show_two(y_test_ori, y_pred_ori)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAADnCAYAAABYHII5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQeElEQVR4nO3de2jV9R/H8feZOo9zq5wTdYpb84KWWymGoeUuTEslcxYYTVGSvJapoHYhrCzTtKx0KF5QGl7wfiNMkU28UYo7EDjF1KPmLkZTtNKh2/v3R3X4rX2nc2c77zO/zwcMPOf7/c7PF17wOud7vnsfj6oKAAChFmG9AACAO1FAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATDS918YWLVqU3L59u22oFoPGwev1Vt6+fZsXL6iCXMCJ1+stvXXrVjunbR5VrfFAj8ej99reGK1du1beeust+eOPP6yX0mh5PB4hF/ivhy0XZKJ+/JMLj9O2RvVqpbS0VN555x3p3LmzNG/eXDp06CCDBw+W77//3npp9aa4uFhef/116d69uzRp0kTGjh1rvaSw54ZcbNu2TQYNGiRt2rSRmJgY6du3r+zatct6WWHLDZk4ePCg9OvXT1q3bi0tWrSQ7t27y6JFi6yX9UDueQkunPj9funfv7/ExMTI559/Lk899ZRUVlbKgQMHZOLEiXLp0iXrJdaL8vJyiYuLk3fffVdWrFhhvZyw55ZcHDx4UDIyMuTTTz+V2NhYWbdunWRlZUl+fr48//zz1ssLK27JRHR0tEydOlWSk5MlKipKjhw5IhMmTJCoqCiZPHmy9fJqR1Vr/Pl7c3gYPHiwxsfH682bN6ttu3btWuDfFy9e1OHDh2t0dLRGR0drVlaWXr58ObB9zZo12rJly8DjOXPm6JNPPlnl99W0z9q1azUhIUGjoqJ07NixWl5erjk5OdqxY0eNjY3V6dOna0VFReC4hIQEnTt3ro4fP15jYmK0Q4cO+sUXX9T6nIcOHapjxoyp9f6hQi6q7hPqXPzrmWee0RkzZjzwcQ0lXHLh5kxkZWXpa6+99sDHNaR/cuHYMY3iElxZWZns3btXpkyZItHR0dW2P/bYYyIiUllZKS+//LKUlpZKXl6e5OXlSVFRkQwfPjzoa9N+v1927twpe/bskW3btsnmzZtl2LBhcvz4cdm3b5+sWrVKlixZItu3b69y3OLFiyU5OVlOnjwps2fPllmzZsmxY8eCWgv+5vZc3Lx5U1q1ahXU+h82bs5EQUGBHD16VFJTU4Naf0jV1EwaRu+AfvzxRxUR3bZt2z3327dvn0ZEROiFCxcCz507d049Ho/u379fVev+qsbr9er169cDz73yyisaFxen5eXlgedSU1N1ypQpgccJCQnVXo106dJF586dW4uz5h3Q/bg1F6qqS5cu1ejoaPX7/bU+pqGFQy7cmIkOHTpoZGSkRkRE6Mcff3zf/UNNGvs7IK3lK5LCwkKJj4+XxMTEwHNJSUkSHx8vp06dCmoNnTp1kkcffTTwuG3bttKtWzeJjIys8tzVq1erHJeSklLlcXx8fLV9UDduzcXWrVtl5syZsn79eklISAhi9Q8fN2bi0KFDcuLECVm+fLl8/fXXkpubG9T6Q6lRFFDXrl3F4/FIYWFhnX+Hx+N4F6BERERUC+2dO3eq7desWbNqv8/pucrKyvse9999UDduzMWWLVtk9OjR8t1338lLL7103/3dxo2ZePzxxyU5OVnefPNNmTFjhnz00Uf3PSZcNIoCio2NlRdeeEGWLl3qeE/+9evXRUSkR48eUlRUJH6/P7Dt/PnzUlRUJE888YTj727Tpo2UlpZWCZbP56vP5aOBuC0XmzZtktGjR8vatWvl1VdfNV1LuHJbJv6rsrJSysvLrZdRa42igEREcnJyRFWlT58+snnzZjlz5oycPn1ali1bFnjrmpmZKSkpKZKdnS0nTpyQEydOSHZ2tvTu3VsyMjIcf29aWpqUlZXJvHnz5Ny5c7J69WrZsmVLKE+tGp/PJz6fT27cuCFlZWXi8/mCvizwsHJLLjZu3CjZ2dkyf/58GTBggJSUlEhJSYmUlZWZrSlcuSUTS5YskT179sjZs2fl7Nmzsnr1alm0aJGMGjXKbE0PqtEUUFJSkpw8eVIGDhwos2fPlpSUFMnIyJBdu3YF/l7G4/HIzp07pU2bNpKeni7p6enSrl072bFjR41vq3v06CHLli2TFStWSEpKiuzfv1/ef//9UJ5aNb169ZJevXrJoUOHZPfu3dKrVy8ZMmSI6ZrClVtysXz5crl7965MmzZN2rdvH/gZMWKE2ZrClVsyUVFRIbNnz5ann35a+vTpIzk5OTJ//nyZN2+e2ZoelOtG8SB4D9vIFdQPcgEnD80oHgDAw4MCAgCYuOcsOK/XW+nxeCgpVOH1emu8Tg73Ihdw4vV6a7yXnM+A8MC41g8n5AJO+AzIwNixY8Xj8YjH45GmTZtKp06dZNKkSXLt2rWQruOzzz6T/v37S8uWLXl1GgbIBZy4NRcUUAPKzMyU4uJi8fv9smrVKtm9e3fIx6SXl5fLiBEjZNq0aSH9f1EzcgEnrsxFTUPiNIyGkTZGY8aM0aFDh1Z5bsaMGRobGxt4XFFRoZ988ol27NhRIyMjtWfPnrpjx47A9pEjR+qECRMCjz/44AMVET127FjguY4dO2pubu5917N58+Z6GxZJLuqOXMCJC3LReIeRPgzOnz8ve/furTLv6ZtvvpGFCxfKggUL5Oeff5asrCwZMWJEYLxHWlqa5OfnB/bPz8+XuLi4wHO//PKL/Prrr5KWlha6E0G9Ihdw4ppc1NRMyjugoIwZM0abNGmiLVu2VK/XqyKiIqJfffVVYJ/4+Phq49NTU1M1OztbVVULCwtVRLSoqEj//PNPjYyM1Pnz5+ugQYNUVXXlypXauXPnWq2HV7rhgVzAiQtywTugUBswYID4fD756aef5O2335YhQ4bI1KlTRUTkxo0bUlRUJP37969yzHPPPReY+9a9e3dp166d5Ofny9GjR6Vz584ycuRIOXLkiNy5c0fy8/PD69UMaoVcwIkbc0EBNaCoqCjp0qWLJCcny7fffit//fWXzJ07977H/f/dJ6mpqZKXlyf5+fmSnp4uiYmJEhcXJ8ePH5eDBw+GXaBwf+QCTtyYCwoohObMmSMLFiyQoqIieeSRRyQ+Pl6OHDlSZZ/Dhw9XGQeflpYWCNS/4UlLS5OVK1eG3/Vc1Am5gBNX5KKma3PKZ0BBcbqrRVW1d+/eOmnSJFVVXbx4scbExOj69ev1zJkz+uGHH2pERIT6fL7A/v9e123WrJlevXpVVf/+GuCmTZvW6nruxYsXtaCgQBcuXKgiogUFBVpQUKA3b96s87mRi7ojF3Diglw4d0xNG5QCCkpNgVq3bp1GRkaq3++vcltls2bNtGfPnrp9+/Zqx7Rr167Kd9FfuHBBRUTHjRtXq3XIPx9o/v9PXl5enc+NXNQduYATF+TCsWMYxYMHxsgVOCEXcMIoHgBA2GEaNh4YU4/hhFzACdOwUa+41AIn5AJOuARnIBym2/r9fhk3bpwkJSVJixYtJCkpSd577z25detWyNaAqsgFnLg1F/e8BIfgZGZmSm5urty9e1dOnTolb7zxhly/fl02bNgQkv//9OnTUlFRIcuWLZOuXbtKYWGhjB8/Xn7//XdZsWJFSNaA6sgFnLgyFzXdHqfchh2UcJtu+6+cnJwqa6gLclF35AJOXJALZsFZCpfptjdu3JBWrVrVwxmhPpALOHFNLmpqJuUdUFDCbbqtqqrf79fWrVvrl19+GdS5kYu6Ixdw4oJc8A4o1MJpum1paam8+OKLMnDgQJk+fXq9niceDLmAEzfmggJqQOEy3bakpETS09OlZ8+ekpuby99qGCMXcOLGXFBAIWQx3ba4uFjS0tKkR48esmHDBmnalBsfww25gBNX5KKma3PKZ0BBCYfptleuXNGuXbtqamqqXrp0SYuLiwM/d+/erfO5kYu6Ixdw4oJcMA07lMJhuu2aNWscJ9uKiF64cKHO50Yu6o5cwIkLcsE0bNQPRq7ACbmAE0bxAADCDtOw8cCYegwn5AJOmIaNesWlFjghF3DCJTgAQNihgBpIOIxXr6yslGHDhkmnTp3E6/VK+/btZdSoUXLlypWQrQFVkQs4cWsuKKAGlJmZKcXFxeL3+2XVqlWye/dumTx5ckjXkJGRIZs2bZIzZ87I1q1b5fz585KVlRXSNaAqcgEnrsxFTfdnK38HFJRwHa++c+dOFRG9detWXU5LVfl7j2CQCzhxQS4YRmopHMarl5WVybp166Rv377i9Xrr6cwQDHIBJ67JRU3NpLwDCko4jVefNWuWRkVFqYjos88+q7/99ltQ50Yu6o5cwIkLcsE7oFALl/HqM2fOlIKCAtm3b580adJERo0axe2yhsgFnLgxFxRQAwqX8epxcXHSrVs3GThwoGzcuFF++OEHOXz4cDCnhiCQCzhxYy4ooBCyGK/+X5WVf/9Rcnl5eVDngvpDLuDEFbmo6dqc8hlQUMJhvPrRo0d16dKl6vP51O/364EDB7Rfv36amJjI3U5GyAWcuCAXfB1DKIXDePWCggJNS0vT2NhYbd68uSYmJurEiRP18uXLQZ0buag7cgEnLsgFX8eA+sHMLzghF3DCLDgAQNi539cxlHo8nrahWgwaB76mA07IBZx4vd7Smrbd8xIcAAANhVcrAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAAT/wOoF0ABHITI/AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
